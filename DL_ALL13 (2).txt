1. Demonstrate use of TENSORFLOW and PYTORCH by implementing simple code in python
import tensorflow as tf
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.utils import to_categorical


# Load data from CSV
data = pd.read_csv('mnist_784_csv.csv')


# Separate features and labels
X = data.iloc[:, 1:].values  # All columns except the first one (pixel values)
y = data.iloc[:, 0].values   # First column (labels)


# Split into training and test sets (e.g., 80% train, 20% test)
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Normalize pixel values
x_train = x_train / 255.0
x_test = x_test / 255.0


# Reshape to 28x28 images (optional)
x_train = x_train.reshape(-1, 28, 28)
x_test = x_test.reshape(-1, 28, 28)


# Convert labels to categorical one-hot encoding
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)


# Build the model
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])


# Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
# Train the model
model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)
# Evaluate the model
loss, accuracy = model.evaluate(x_test, y_test)
print(f'Test accuracy: {accuracy:.2f}')
________________


import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset


# Load data from CSV
data = pd.read_csv('mnist_784_csv.csv')


# Separate features and labels
X = data.iloc[:, 1:].values  # All columns except the first one (pixel values)
y = data.iloc[:, 0].values   # First column (labels)


# Split into training and testing sets (e.g., 80% train, 20% test)
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Convert to PyTorch tensors and normalize pixel values
x_train = torch.tensor(x_train / 255.0, dtype=torch.float32)
x_test = torch.tensor(x_test / 255.0, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
y_test = torch.tensor(y_test, dtype=torch.long)


# Create DataLoader for batching
train_data = TensorDataset(x_train, y_train)
test_data = TensorDataset(x_test, y_test)
train_loader = DataLoader(train_data, batch_size=32, shuffle=True)
test_loader = DataLoader(test_data, batch_size=32)


# Define the model
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(28 * 28, 128)
        self.fc2 = nn.Linear(128, 10)


    def forward(self, x):
        x = self.flatten(x)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x


# Initialize the model, loss function, and optimizer
model = SimpleNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)


# Training loop
for epoch in range(5):
    model.train()
    for images, labels in train_loader:
        optimizer.zero_grad()
        output = model(images)
        loss = criterion(output, labels)
        loss.backward()
        optimizer.step()


    print(f'Epoch {epoch + 1}, Loss: {loss.item():.4f}')


# Evaluate the model
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        output = model(images)
        _, predicted = torch.max(output, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()


accuracy = correct / total
print(f'Test accuracy: {accuracy:.2f}')
________________




2. Demonstrate use of TENSORFLOW and KERAS (Diabetes Dataset) by implementing simple code in python
import tensorflow as tf 
from tensorflow import keras 
from tensorflow.keras import layers 
import numpy as np


# 1. Load the Diabetes dataset (assuming it's in a CSV file)
dataset = np.loadtxt('/content/sample_data/diabetes.Csv', delimiter=',', skiprows=1)
X = dataset[:, 0:8]  # Features
y = dataset[:, 8]  # Target (diabetes outcome )


# 2. Split the dataset into training and testing sets 
from sklearn.model_selection import train_test_split
X_train, X_test, y _train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# 3. Create a simple Keras model
model = keras.Sequential([
layers.Dense(12, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer with 12 neurons layers. Dense(8, activation='relu'),  # Hidden layer with 8 neurons
         layers. Dense(1, activation='sigmoid') # Output layer with sigmoid for binary classification


# 4. Compile the model
model.compile(optimizer=’adam’, loss=’binary_crossentropy’, metrics=[‘accuracy’])


#5. Train the model 
model.fit(X_train, y_train, epochs=150, batch_size=10)


#6. Evaluate the model on the test set
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test accuracy: {accuracy * 100:.2f}%")


#7. Make predictions 
predictions = model.predict(X_test)


predicted_probabilities = model.predict (x)


predicted_classes = (predicted probabilities › 0.5) .astype(int)
# Print the first 10 actual and predicted values
print("Actual values:" , y [:10])
print ("Predicted probabilities:", predicted_probabilities[:10])
print("Predicted classes:", predicted_classes [:10])
3. Implement Feedforward neural networks with KERAS and TensorFlow MNIST Digit dataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from keras.datasets import mnist


data = pd.read_csv('Dataset/NEW/3 MNIST/mnist_784_csv.csv')
data.head()


# Separate features and labels
x = data.iloc[:, :-1].values  # All columns except the last one
y = data.iloc[:, -1].values   # The last column


# Split the dataset into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)


x_train = train_data.iloc[:, :-1].values
y_train = train_data.iloc[:, -1].values


x_test = test_data.iloc[:, :-1].values
y_test = test_data.iloc[:, -1].values


x_train = x_train.reshape(-1, 28, 28, 1)
x_test  = x_test.reshape(-1, 28, 28, 1)


y_train = y_train.reshape(-1, 1)
y_test  = y_test.reshape(-1, 1)


for i in (x_train, y_train, x_test, y_test):
    print(i.shape)


plt.imshow(x_train[90])
plt.show()
print(y_train[90])


print(np.unique(y_train))
print(np.unique(y_test))


x_train = x_train / 255
x_test = x_test / 255


print(x_train.shape, x_test.shape)


model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)), 
    keras.layers.Dense(50, activation='relu', name='L1'),
    keras.layers.Dense(50, activation='relu', name='L2'),
    keras.layers.Dense(10, activation='softmax', name='L3')
])
model.compile(optimizer='sgd', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])


history = model.fit(x_train, y_train,
                    batch_size=30, 
                    epochs=10, 
                    validation_data=(x_test, y_test),
                    shuffle=True)


import seaborn as sns
sns.lineplot(model.history.history)


import matplotlib.pyplot as plt


plt.figure(figsize=[15, 8])


# Summarize history for accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['train', 'test'], loc='upper left')


# Summarize history for Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['train', 'test'], loc='upper left')


plt.show()
loss, accuracy = model.evaluate(x_test, y_test)


predicted_value = model.predict(x_test)
plt.imshow(x_test[1111])
plt.show()
print(np.argmax(predicted_value[1111], axis=0))


test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2)
print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")
4. Implement Feedforward neural networks with KERAS and TensorFlow CIFAR image dataset


5. Build image classification model using CNN on fashion MNIST dataset.
import tensorflow as tf
from tensorflow.keras import datasets
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np


train_csv_path = 'Dataset/NEW/5 Fashion MNIST/fashion-mnist_train.csv'
test_csv_path  = 'Dataset/NEW/5 Fashion MNIST/fashion-mnist_test.csv'


train_data = pd.read_csv(train_csv_path)
test_data = pd.read_csv(test_csv_path)


x_train = train_data.iloc[:, 1:].values
y_train = train_data.iloc[:, 0].values
x_test = test_data.iloc[:, 1:].values
y_test = test_data.iloc[:, 0].values


x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255
x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255


y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)


class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']


plt.figure(figsize=(10, 5))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow(x_train[i].reshape(28, 28), cmap='gray')
    plt.title(class_names[y_train[i].argmax()])
    plt.axis('off')
plt.show()


model = models.Sequential()


model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))


model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])


model.summary()


history = model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print(f'\nTest Accuracy: {test_acc:.4f}')


predictions = model.predict(x_test)
predicted_classes = predictions.argmax(axis=1)
true_classes = y_test.argmax(axis=1)




from sklearn.metrics import classification_report
print(classification_report(true_classes, predicted_classes))


plt.figure(figsize=(12, 4))


# Plot training & validation accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Model Accuracy')
plt.legend()




# Plot training & validation loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Model Loss')
plt.legend()


plt.show()


index = 80


true_label = class_names[true_classes[index]]
predicted_label = class_names[predicted_classes[index]]


plt.figure(figsize=[10, 10])
plt.imshow(x_test[index].reshape(28, 28), cmap='gray')
plt.title(f"True Label: {true_label}\nPredicted Value: {predicted_label}", size=20)
plt.axis('off')
plt.show()


6. Build image classification model using CNN on pneumonia X RAY IMAGE dataset.
import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import classification_report


train_dir = 'Dataset/NEW/6 Pneumonia_X_Ray/train'
test_dir = 'Dataset/NEW/6 Pneumonia_X_Ray/test'
Change in 7th //train_dir = 'Dataset/NEW/7 Brain_Tumor/Training'
Change in 7th//test_dir = 'Dataset/NEW/7 Brain_Tumor/Testing'
—-----------------------------------------------------------------------------


IMG_HEIGHT, IMG_WIDTH = 150, 150
BATCH_SIZE = 32


train_datagen = ImageDataGenerator(
    rescale = 1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)


test_datagen = ImageDataGenerator(rescale=1./255)


train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='binary'
)


test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='binary'
)


model = models.Sequential()


model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3,3), activation='relu'))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(128, (3,3), activation='relu'))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Flatten())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid')) // model.add(layers.Dense(4, activation='softmax'))// for the 7th problem statement, same code but change activation function—------------------




model.compile(optimizer='adam',
              loss='binary_crossentropy', // loss='categorical_crossentropy', // for the 7th problem, same but change loss binary crossentropy—-----------------------------
              metrics=['accuracy'])
model.summary()


steps_per_epoch = train_generator.samples // BATCH_SIZE
validation_steps = test_generator.samples // BATCH_SIZE
history = model.fit(
    train_generator,
    steps_per_epoch=steps_per_epoch,
    epochs=10,
    validation_data=test_generator,
    validation_steps=validation_steps
)


test_loss, test_acc = model.evaluate(test_generator, verbose=2)
print("Test Accuracy ", test_acc) //print(f'Test Accuracy: {test_acc:.4f}') for 7th —-----------------------------


predictions = model.predict(test_generator)
predicted_classes = (predictions > 0.5).astype("int32").flatten()
true_classes = test_generator.classes
print(classification_report(true_classes, predicted_classes))
—----------------------------------------------------------------------------------
FOR 7th change from predictions part and then this whole finishes the code till the …. Line :
predictions = model.predict(test_generator)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = test_generator.classes


print(classification_report(true_classes, predicted_classes, target_names=train_generator.class_indices.keys()))


cm = confusion_matrix(true_classes, predicted_classes)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=train_generator.class_indices.keys(), yticklabels=train_generator.class_indices.keys())
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.title('Confusion Matrix')
plt.show()


def plot_predictions(generator, num_images=20):
    images, labels = next(generator)
    predictions = model.predict(images)
    predicted_classes = np.argmax(predictions, axis=1)
    true_classes = np.argmax(labels, axis=1)
    
    class_names = list(generator.class_indices.keys())
    plt.figure(figsize=(15, 5))
    for i in range(num_images):
        plt.subplot(2, num_images // 2, i + 1)
        plt.imshow(images[i])
        plt.title(f'True: {class_names[true_classes[i]]} \nPred: {class_names[predicted_classes[i]]}')
        plt.axis('off')
    plt.tight_layout()
    plt.show()
plot_predictions(test_generator)
—-----------------------------------------------------------------
Cont.. for 6th from here again 
plt.figure(figsize=(12, 4))
# Plot training & validation accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Model Accuracy')
plt.legend()
# Plot training & validation loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Model loss')
plt.legend()
plt.show()


def plot_predictions(generator, num_images=20):
    images, labels = next(generator)
    
    predictions = model.predict(images)
    predicted_classes = (predictions > 0.5).astype("int32").flatten() 
    
    true_classes = labels.flatten()
    
    plt.figure(figsize=(15, 5))
    for i in range(num_images):
        plt.subplot(2, num_images // 2, i + 1)
        plt.imshow(images[i])
        plt.title(f'True: {true_classes[i]} \nPred: {predicted_classes[i]}')
        plt.axis('off')
    
    plt.tight_layout()
    plt.show()
plot_predictions(test_generator)


7. Build Brain tumor classification model with CNN (INCLUDED IN 6th with changes) highlighted in blue 
8. Build Recurrent Neural Network by using the NUMPY library


import numpy as np


# Initialize Parameters
input_size = 2
hidden_size = 3
output_size = 1
Wx = np.random.rand(hidden_size, input_size) * 0.01
Wh = np.random.rand(hidden_size, hidden_size) * 0.01
Wy = np.random.rand(output_size, hidden_size) * 0.01
bh = np.zeros((hidden_size, 1))
by = np.zeros((output_size, 1))


# Define Activation Function
def tanh(x):
return np.tanh(x)


def tanh_derivative(x):
return 1 - np.tanh(x) ** 2


# Forward Pass
def rnn_forward(inputs):
h = np.zeros((hidden_size, 1))
for x in inputs:
x = x.reshape(-1, 1)
h = tanh(np.dot(Wx, x) + np.dot(Wh, h) + bh)
y = np.dot(Wy, h) + by
return y, h


# Backward Pass
def rnn_backward(inputs, target, y, h):
loss = (y - target) ** 2
dWy = (y - target) * h.T
dby = (y - target)
dh = np.dot(Wy.T, (y - target))


dWx_total = np.zeros_like(Wx)
dWh_total = np.zeros_like(Wh)
dbh_total = np.zeros_like(bh)


for t in reversed(range(len(inputs))):
x = inputs[t].reshape(-1, 1)
dhraw = dh * tanh_derivative(h)
dWx = np.dot(dhraw, x.T)
dWh = np.dot(dhraw, h.T)
dbh = dhraw
dh = np.dot(Wh.T, dhraw)


dWx_total += dWx
dWh_total += dWh
dbh_total += dbh


return dWx_total, dWh_total, dWy, dbh_total, dby


# Update Weights
def update_parameters(learning_rate, dWx, dWh, dWy, dbh, dby):
global Wx, Wh, Wy, bh, by
Wx -= learning_rate * dWx
Wh -= learning_rate * dWh
Wy -= learning_rate * dWy
bh -= learning_rate * dbh
by -= learning_rate * dby


inputs = [np.array([1, 0]), np.array([0, 1]), np.array([1, 1])]
target = np.array([[1]])


learning_rate = 0.01
for epoch in range(100):
y, h = rnn_forward(inputs)
dWx, dWh, dWy, dbh, dby = rnn_backward(inputs, target, y, h)
update_parameters(learning_rate, dWx, dWh, dWy, dbh, dby)
if epoch % 10 == 0:
print(f'Epoch {epoch}, Loss: {np.mean((y - target) ** 2)}')




________________




9. Use Autoencoder to implement anomaly detection on credit card dataset


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from keras.models import Model, Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras import layers, models
import tensorflow as tf


df = pd.read_csv('Dataset/NEW/9 Credit Card Dataset/creditcard.csv')
df.head()


df = df.drop(['Time', 'Class'], axis=1)


x_train, x_test = train_test_split(df, test_size=0.2)


# Encoder


encoder = tf.keras.models.Sequential([
    layers.Input(shape=(x_train.shape[1],)),
    layers.Dense(64, activation='relu'),
    layers.Dense(64, activation='relu'),
    layers.Dense(20, activation='relu')
])


# Decoder


decoder = tf.keras.models.Sequential([
    layers.Input(shape=(20, )),
    layers.Dense(64, activation='relu'),
    layers.Dense(64, activation='relu'),
    layers.Dense(x_train.shape[1], activation='linear')
])












# Autoencoder
autoencoder = tf.keras.models.Sequential([
    encoder, 
    decoder
])


autoencoder.compile(optimizer='adam', loss='mean_squared_error')




history = autoencoder.fit(
    x_train, x_train,
    validation_data = (x_test, x_test),
    epochs=10,
    batch_size=100,
    shuffle=True
)


import seaborn as sns
sns.lineplot(autoencoder.history.history)


predictions = autoencoder.predict(x_test)
mse = np.mean(np.power(x_test - predictions, 2), axis=1)


threshold = np.percentile(mse, 95)
threshold


anomalies = mse > threshold
print("Number of ANomalies -> ", np.sum(anomalies)) 


import matplotlib.pyplot as plt


plt.plot(mse, marker='o', linestyle='', markersize=3, label='MSE', )
plt.axhline(threshold, color='r', linestyle='--', label='Anomaly Threshold')
plt.xlabel('Sample Index')
plt.ylabel('MSE')
plt.title('Anomaly Detection Results')
plt.legend()
plt.show()




10. Implement the concept of image denoising using autoencoders on MNIST data set


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras import layers


file = "Dataset/NEW/3 MNIST/mnist_784_csv.csv"


data = pd.read_csv(file)
data.head()


x = data.iloc[:, :-1].values
y = data.iloc[:, -1].values


x = np.reshape(x, (len(x), 28, 28, 1))


from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)


x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0


def add_noise(images):
    noise_factor = 0.5
    noisy_images = images + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=images.shape)
    noisy_images = np.clip(noisy_images, 0., 1.)
    return noisy_images
x_train_noisy = add_noise(x_train)
x_test_noisy  = add_noise(x_test)


# autoencoder model
autoencoder = keras.models.Sequential([
    layers.Input(shape=(28, 28, 1)),
    layers.Conv2D(16, (3, 3), activation='relu', padding='same'),
    layers.MaxPooling2D((2, 2), padding='same'),
    layers.Conv2D(8, (3, 3), activation='relu', padding='same'),
    layers.MaxPooling2D((2, 2), padding='same'),
    layers.Conv2D(8, (3,3 ), activation='relu', padding='same'),
    layers.UpSampling2D((2, 2)),
    layers.Conv2D(16, (3, 3), activation='relu', padding='same'),
    layers.UpSampling2D((2, 2)),
    layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')
])


# Compile the autoencoder
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')


# Fit the model using noisy images as input
history = autoencoder.fit(x_train_noisy, x_train,
                          epochs=50, 
                          batch_size=256,
                          shuffle=True,
                          validation_data=(x_test_noisy, x_test))


loss = autoencoder.evaluate(x_test_noisy, x_test)
print(loss)


denoised_images = autoencoder.predict(x_test_noisy)
def display(original, noisy, denoised, n=10):
    plt.figure(figsize=(20, 6))
    for i in range(n):
        ax = plt.subplot(3, n, i+1)
        plt.imshow(original[i].reshape(28, 28), cmap='gray')
        plt.title("Original")
        plt.axis("off")
        ax = plt.subplot(3, n, i+1+n)
        plt.imshow(noisy[i].reshape(28, 28), cmap='gray')
        plt.title("Noisy")
        plt.axis("off")
        ax = plt.subplot(3, n, i+1+2*n)
        plt.imshow(denoised[i].reshape(28, 28), cmap='gray')
        plt.title("Denoised")
        plt.axis("off")   
    plt.show()
display(x_test, x_test_noisy, denoised_images)




11. Implement object detection using Transfer learning on flower data set
import os
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import ResNet50
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing import image
from sklearn.metrics import classification_report


train_dir = 'Dataset/NEW/11 Flowers/train'
test_dir = 'Dataset/NEW/11 Flowers/test'


train_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)


train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)


test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)


# Load pre-trained ResNet50 model
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))


#FREEZE THE base model layers
for layer in base_model.layers:
    layer.trainable = False


# add custom classifier
model = models.Sequential()
model.add(base_model)
model.add(layers.GlobalAveragePooling2D())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(train_generator.num_classes, activation='softmax'))


model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])


#train the model
history = model.fit(train_generator,
                    epochs=1,
                    validation_data=test_generator)


# Optionally, fine-tune more layers 
# Unfreeze the last few layers of the base model


for layer in base_model.layers[-20:]:
    layer.trainable = True


model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])


history_fine = model.fit(train_generator,
                         epochs=1,
                         validation_data=test_generator)


test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test accuracy: {test_accuracy:.2f}')
print(f'Test loss: {test_loss:.2f}')


test_generator.reset()
predictions = model.predict(test_generator)
predicted_classes = np.argmax(predictions, axis=1)


true_classes = test_generator.classes
class_labels = list(test_generator.class_indices.keys())


print(classification_report(true_classes, predicted_classes, target_names=class_labels))
  img = image.load_img(img_path, target_size=(224, 224))
    img_arr = image.img_to_array(img)
    img_arr = np.expand_dims(img_arr, axis=0)
    img_arr /= 255.0
    predictions = model.predict(img_arr)
    predicted_class = class_labels[np.argmax(predictions[0])]
    plt.imshow(img)
    plt.title(f'Predicted : {predicted_class}')
    plt.axis('off')
    plt.show()
test_image_path = 'Dataset/NEW/11 Flowers/test/roses/roses_112.jpg'
predict_image(test_image_path)


test_image_path = 'Dataset/NEW/11 Flowers/test/tulips/tulips_509.jpg'
predict_image(test_image_path)
12. Implement image classification using transfer learning on animal dataset
import os
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import ResNet50
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing import image




data_dir = "Dataset/NEW/12 Animal/raw-img"
class_names = os.listdir(data_dir)
class_names


train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)
# Split data for training and validation
train_generator = train_datagen.flow_from_directory(
    data_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    subset='training'
)
validation_generator = train_datagen.flow_from_directory(
    data_dir, 
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    subset='validation'
)


#load pre trained resnet
model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))


#FREEZE THE base model layers
for layer in base_model.layers:
    layer.trainable = False


# add custom classifier
model = models.Sequential()
model.add(base_model)
model.add(layers.GlobalAveragePooling2D())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(train_generator.num_classes, activation='softmax'))


model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])


#train the model
history = animal_model.fit(train_generator,
                           validation_data = validation_generator,
                           epochs=1)


# Optionally, fine-tune more layers
# Unfreeze the last few layers of the base model


for layer in model.layers[-20:]:
    layer.trainable = True




# Recompile the model after changes
animal_model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])


history_fine = animal_model.fit(train_generator, 
                         validation_data=validation_generator, 
                         epochs=1)


def prediction(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_arr = image.img_to_array(img)
    img_arr = np.expand_dims(img_arr, axis=0)
    img_arr /= 255.0


    predictions = animal_model.predict(img_arr)
    predicted_class = class_names[np.argmax(predictions[0])]


    plt.imshow(img)
    plt.title(f'Predicted: {predicted_class}')
    plt.axis('off')
    plt.show()


test_image_path = 'Dataset/NEW/12 Animal/raw-img/ragno/e83cb2082bfd013ed1584d05fb1d4e9fe777ead218ac104497f5c97ca5ecb5b1_640.jpg'
prediction(test_image_path)




test_image_path = 'Dataset/NEW/12 Animal/raw-img/elefante/e133b10d2bf21c22d2524518b7444f92e37fe5d404b0144390f8c07aa4e5b0_640.jpg'
prediction(test_image_path)




















13. Implement the Continuous Bag of Words (CBOW) Model.


import numpy as np
import pandas as pd
import re
from tensorflow.keras.preprocessing.text import Tokenizer
import seaborn as sns
from sklearn.decomposition import PCA
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Lambda


data = """Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance."""


data


sentences = data.split('.')
sentences


clean_sent = []
for sentence in sentences:
    if sentence == "":
        continue
    sentence = re.sub('[^A-Za-z0-9]+', ' ', (sentence))
    sentence = re.sub(r'(?:^|)\w (?:$| )', ' ', (sentence)).strip()
    sentence = sentence.lower()
    clean_sent.append(sentence)
clean_sent


tokenizer = Tokenizer()
tokenizer.fit_on_texts(clean_sent)
sequences = tokenizer.texts_to_sequences(clean_sent)
sequences


index_to_word = {}
word_to_index = {}
for i, sequence in enumerate(sequences):
    word_in_sentence = clean_sent[i].split()
    for j, value in enumerate(sequence):
        index_to_word[value] = word_in_sentence[j]
        word_to_index[word_in_sentence[j]] = value
print(index_to_word)
print("\n")
print(word_to_index)


vocab_size = len(tokenizer.word_index) + 1
emb_size = 10
context_size = 2
contexts = []
targets = []
for sequence in sequences:
    for i in range(context_size, len(sequence) - context_size):
        target = sequence[i]
        context = [sequence[i-2], sequence[i-1], sequence[i+1], sequence[i+2]]
        contexts.append(context)
        targets.append(target)
print(contexts)
print("\n")
print(targets)


for i in range(10):
    word = []
    target = index_to_word.get(targets[i])
    for j in contexts[i]:
        word.append(index_to_word.get(j))         
 print(word, " -> ", target)
x = np.array(contexts)
y = np.array(targets)




model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=emb_size, input_length=2*context_size),
    Lambda(lambda x: tf.reduce_mean(x, axis=1)),
    Dense(256, activation='relu'),
    Dense(512, activation='relu'),
    Dense(vocab_size, activation='softmax')
])


model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])


history = model.fit(x, y, epochs=80)




sns.lineplot(model.history.history)


embeddings = model.get_weights()[0]
pca = PCA(n_components=2)
reduced_embeddings = pca.fit_transform(embeddings)


data


test_sentences = [
    "known as structured learning",
    "transformers have applied to",
    "where they produced results",
    "cases surpassing expert performance"
]


   test_words = sent.split(" ")
    x_test = []
    for word in test_words:
        x_test.append(word_to_index.get(word))
    x_test = np.array([x_test])
    pred = model.predict(x_test)
    pred = np.argmax(pred[0])
    print("Pred ", test_words, "\n = ", index_to_word.get(pred), "\n\n")




________________


4…..Implement Feedforward neural networks with KERAS and TensorFlow CIFAR image dataset


import os
import shutil
import numpy as np
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt


# Set the directory path where the CIFAR-10 dataset is stored
original_dataset_dir = 'C:\\Users\\ochau\\Downloads\\cifar10_dataset\\cifar10_dataset'
train_dir = 'C:\\Users\\ochau\\Downloads\\cifar10_dataset\\train'
test_dir = 'C:\\Users\\ochau\\Downloads\\cifar10_dataset\\test'


# Create training and testing directories if they do not exist
os.makedirs(train_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)


# Split the dataset into training and testing sets
def split_dataset(original_dataset_dir, train_dir, test_dir, split_ratio=0.8):
    classes = os.listdir(original_dataset_dir)
    for class_name in classes:
        class_dir = os.path.join(original_dataset_dir, class_name)
        if os.path.isdir(class_dir):
            # Create class directories in train and test folders
            os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)
            os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)


            # Get all image files in the class directory
            images = os.listdir(class_dir)
            np.random.shuffle(images)  # Shuffle images
            split_index = int(len(images) * split_ratio)


            # Move images to train directory
            for img in images[:split_index]:
                shutil.move(os.path.join(class_dir, img), os.path.join(train_dir, class_name, img))


            # Move images to test directory
            for img in images[split_index:]:
                shutil.move(os.path.join(class_dir, img), os.path.join(test_dir, class_name, img))


# Call the function to split the dataset
split_dataset(original_dataset_dir, train_dir, test_dir)


# Image dimensions
img_height, img_width = 32, 32
batch_size = 32


# Data augmentation and preprocessing
datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)


# Load training data
train_data = datagen.flow_from_directory(
    train_dir,
    target_size=(img_height, img_width),
    color_mode='rgb',
    class_mode='categorical',  # use 'categorical' for one-hot encoding
    batch_size=batch_size,
    shuffle=True,
    seed=42
)


# Load testing data
test_data = datagen.flow_from_directory(
    test_dir,
    target_size=(img_height, img_width),
    color_mode='rgb',
    class_mode='categorical',  # use 'categorical' for one-hot encoding
    batch_size=batch_size,
    shuffle=False
)


# Check the number of classes
num_classes = len(train_data.class_indices)
print(f'Number of classes: {num_classes}')
print('Class indices:', train_data.class_indices)


# Build the model
model = keras.Sequential()
model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)))
model.add(keras.layers.MaxPooling2D((2, 2)))
model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))
model.add(keras.layers.MaxPooling2D((2, 2)))
model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))
model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(64, activation='relu'))
model.add(keras.layers.Dense(num_classes, activation='softmax'))  # Change to num_classes
model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])


# Fit the model
history = model.fit(train_data, epochs=10, validation_data=test_data)


# Plotting the training history
plt.figure(figsize=[12, 5])
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Test Accuracy')
plt.title('Training and Testing Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()


plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Test Loss')
plt.title('Training and Testing Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()


plt.show()